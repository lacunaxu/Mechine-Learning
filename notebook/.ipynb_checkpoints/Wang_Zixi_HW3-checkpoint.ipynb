{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac37034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#import os\n",
    "\n",
    "#files  = zipfile.ZipFile('AReM.zip', 'r')\n",
    "#files.extractall('552_hw3')\n",
    "#files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8155cd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data: 19 files\n",
      "Train Data: 69 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "folder_paths = [\"bending1\", \"bending2\", \"cycling\", \"lying\", \n",
    "                \"sitting\", \"standing\", \"walking\"]\n",
    "\n",
    "columns = ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', \n",
    "           'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "expected_columns = 7\n",
    "problematic_lines = []\n",
    "\n",
    "def detect_separator(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for _ in range(5):\n",
    "            next(file)\n",
    "        sample = file.read(200)  \n",
    "    \n",
    "    if ',' in sample:\n",
    "        return ','\n",
    "    else:\n",
    "        return '\\s+' \n",
    "\n",
    "def process_file(file_path):\n",
    "    sep = detect_separator(file_path)\n",
    "    df = pd.read_csv(file_path, comment='#', header=None, usecols=range(7), sep=sep)\n",
    "    df.columns = columns\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "for folder in folder_paths:\n",
    "    overall_path = os.getcwd()\n",
    "    path = f\"{overall_path}/AReM/{folder}/\" \n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    for idx, file in enumerate(files):\n",
    "        \n",
    "        file_path = os.path.join(path, file)    \n",
    "        df = process_file(file_path)\n",
    "        if folder == \"bending1\" or folder == \"bending2\":\n",
    "            if idx < 2:\n",
    "                test_data.append(df)\n",
    "            else:\n",
    "                train_data.append(df)\n",
    "        else:\n",
    "            if idx < 3:\n",
    "                test_data.append(df)\n",
    "            else:\n",
    "                train_data.append(df)\n",
    "\n",
    "print(f\"Test Data: {len(test_data)} files\")\n",
    "print(f\"Train Data: {len(train_data)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e03a2e",
   "metadata": {},
   "source": [
    "2(i)\n",
    " Research what types of time-domain features are usually used in time series classification and list them (examples are minimum, maximum, mean, etc)\n",
    " \n",
    "Based on my research:\n",
    "\"In time series classification, time-domain features are extracted directly from the raw time series data. These features help capture the statistical properties and patterns of the data over time.\"\n",
    "\n",
    "Here are some time-domain features:\n",
    "Mean, Standard Deviation, Minimum and Maximum values, range, median, skewness, quartiles and IQR, Root Mean Square, Absolute Mean Difference, Entropy, trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bcb8390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_1  max_1  mean_1  median_1  std_1  1st_quart_1  3rd_quart_1  min_2  \\\n",
      "0  33.00  47.75   42.18     43.50   3.67        39.15        45.00    0.0   \n",
      "1  33.00  45.75   41.68     41.75   2.24        41.33        42.75    0.0   \n",
      "2  37.25  45.00   40.62     40.50   1.48        39.25        42.00    0.0   \n",
      "3  38.00  45.67   42.81     42.50   1.44        42.00        43.67    0.0   \n",
      "4  35.00  47.40   43.95     44.33   1.56        43.00        45.00    0.0   \n",
      "\n",
      "   max_2  mean_2  ...  std_5  1st_quart_5  3rd_quart_5  min_6  max_6  mean_6  \\\n",
      "0   3.00    0.70  ...   3.85        30.46        36.33    0.0   2.18    0.61   \n",
      "1   2.83    0.54  ...   2.41        28.46        31.25    0.0   1.79    0.38   \n",
      "2   1.30    0.36  ...   2.19        33.00        36.00    0.0   1.92    0.57   \n",
      "3   1.22    0.37  ...   2.00        32.00        34.50    0.0   3.11    0.57   \n",
      "4   1.70    0.43  ...   2.00        35.36        36.50    0.0   1.79    0.49   \n",
      "\n",
      "   median_6  std_6  1st_quart_6  3rd_quart_6  \n",
      "0      0.50   0.52          0.0         1.00  \n",
      "1      0.43   0.39          0.0         0.50  \n",
      "2      0.43   0.58          0.0         1.30  \n",
      "3      0.43   0.60          0.0         1.30  \n",
      "4      0.43   0.51          0.0         0.94  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "#2(ii)\n",
    "def extract_features(df):\n",
    "    features = {}\n",
    "    for i, col in enumerate(df.columns[1:]): \n",
    "        features[f'min_{i+1}'] = df[col].min()\n",
    "        features[f'max_{i+1}'] = df[col].max()\n",
    "        features[f'mean_{i+1}'] = df[col].mean()\n",
    "        features[f'median_{i+1}'] = df[col].median()\n",
    "        features[f'std_{i+1}'] = df[col].std()\n",
    "        features[f'1st_quart_{i+1}'] = df[col].quantile(0.25)\n",
    "        features[f'3rd_quart_{i+1}'] = df[col].quantile(0.75)\n",
    "    return features\n",
    "\n",
    "train_features = pd.DataFrame([extract_features(df) for df in train_data])\n",
    "test_features = pd.DataFrame([extract_features(df) for df in test_data])\n",
    "\n",
    "total_features = pd.concat([train_features, test_features], axis=0).round(2)\n",
    "print(total_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3b1c88e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Normalized Features DataFrame:\n",
      "\n",
      "   min_1  max_1  mean_1  median_1  std_1  1st_quart_1  3rd_quart_1  min_2  \\\n",
      "0   0.69   0.68    0.75      0.81   0.48         0.64         0.67    0.0   \n",
      "1   0.69   0.60    0.73      0.74   0.29         0.73         0.59    0.0   \n",
      "2   0.78   0.57    0.69      0.68   0.19         0.65         0.56    0.0   \n",
      "3   0.79   0.60    0.78      0.77   0.18         0.76         0.62    0.0   \n",
      "4   0.73   0.66    0.83      0.85   0.20         0.80         0.67    0.0   \n",
      "\n",
      "   max_2  mean_2  ...  std_5  1st_quart_5  3rd_quart_5  min_6  max_6  mean_6  \\\n",
      "0   0.15    0.15  ...   0.34         0.86         0.99    0.0   0.03    0.07   \n",
      "1   0.14    0.12  ...   0.11         0.80         0.83    0.0   0.00    0.00   \n",
      "2   0.05    0.08  ...   0.07         0.93         0.98    0.0   0.01    0.06   \n",
      "3   0.05    0.08  ...   0.04         0.90         0.94    0.0   0.11    0.06   \n",
      "4   0.08    0.09  ...   0.04         1.00         1.00    0.0   0.00    0.03   \n",
      "\n",
      "   median_6  std_6  1st_quart_6  3rd_quart_6  \n",
      "0      0.02   0.08          0.0         0.12  \n",
      "1      0.00   0.00          0.0         0.00  \n",
      "2      0.00   0.12          0.0         0.19  \n",
      "3      0.00   0.13          0.0         0.19  \n",
      "4      0.00   0.08          0.0         0.11  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "Z-score Normalized Features DataFrame:\n",
      "   min_1  max_1  mean_1  median_1  std_1  1st_quart_1  3rd_quart_1  min_2  \\\n",
      "0   0.42   0.52    0.62      0.86   0.39         0.35         0.74    0.0   \n",
      "1   0.42   0.06    0.52      0.53  -0.43         0.70         0.30    0.0   \n",
      "2   0.86  -0.11    0.32      0.30  -0.86         0.36         0.16    0.0   \n",
      "3   0.94   0.05    0.73      0.67  -0.88         0.81         0.48    0.0   \n",
      "4   0.63   0.44    0.95      1.01  -0.81         0.98         0.74    0.0   \n",
      "\n",
      "   max_2  mean_2  ...  std_5  1st_quart_5  3rd_quart_5  min_6  max_6  mean_6  \\\n",
      "0  -0.65   -0.49  ...   0.31         2.74         3.24  -0.11  -1.71   -0.88   \n",
      "1  -0.69   -0.59  ...  -1.11         2.41         2.31  -0.11  -1.87   -1.08   \n",
      "2  -0.99   -0.70  ...  -1.32         3.16         3.18  -0.11  -1.81   -0.91   \n",
      "3  -1.01   -0.70  ...  -1.51         3.00         2.90  -0.11  -1.34   -0.91   \n",
      "4  -0.91   -0.66  ...  -1.51         3.55         3.27  -0.11  -1.87   -0.98   \n",
      "\n",
      "   median_6  std_6  1st_quart_6  3rd_quart_6  \n",
      "0     -0.88  -1.03         -1.2        -0.75  \n",
      "1     -0.94  -1.28         -1.2        -1.08  \n",
      "2     -0.94  -0.91         -1.2        -0.56  \n",
      "3     -0.94  -0.87         -1.2        -0.56  \n",
      "4     -0.94  -1.05         -1.2        -0.79  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler_min_max = MinMaxScaler()\n",
    "normalized_min_max = scaler_min_max.fit_transform(total_features)\n",
    "normalized_data_min_max = pd.DataFrame(normalized_min_max, \n",
    "                                       columns=total_features.columns).round(2)\n",
    "\n",
    "scaler_z_score = StandardScaler()\n",
    "normalized_z_score = scaler_z_score.fit_transform(total_features)\n",
    "normalized_data_z_score = pd.DataFrame(normalized_z_score, \n",
    "                                       columns=total_features.columns).round(2)\n",
    "\n",
    "print(\"Min-Max Normalized Features DataFrame:\\n\")\n",
    "print(normalized_data_min_max.head())\n",
    "\n",
    "print(\"Z-score Normalized Features DataFrame:\")\n",
    "print(normalized_data_z_score.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1fd0439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 90% bootstrap confidence interval for the standard deviation of each feature is:\n",
      "             Standard Deviation  CI Lower Bound  CI Upper Bound  CI Length\n",
      "min_1                    9.5700          8.3957         10.8545     2.4588\n",
      "max_1                    4.3944          3.3320          5.2827     1.9507\n",
      "mean_1                   5.3352          4.7079          5.8833     1.1754\n",
      "median_1                 5.4402          4.7952          6.0118     1.2166\n",
      "std_1                    1.7721          1.5693          1.9436     0.3743\n",
      "1st_quart_1              6.1535          5.5587          6.6631     1.1044\n",
      "3rd_quart_1              5.1390          4.3456          5.8377     1.4921\n",
      "min_2                    0.0000          0.0000          0.0000     0.0000\n",
      "max_2                    5.0627          4.6176          5.4001     0.7825\n",
      "mean_2                   1.5743          1.3841          1.7176     0.3335\n",
      "median_2                 1.4121          1.2414          1.5429     0.3015\n",
      "std_2                    0.8843          0.8033          0.9397     0.1364\n",
      "1st_quart_2              0.9465          0.8299          1.0373     0.2074\n",
      "3rd_quart_2              2.1248          1.8949          2.2921     0.3972\n",
      "min_3                    2.9565          2.7564          3.1046     0.3482\n",
      "max_3                    4.8751          4.1854          5.4826     1.2972\n",
      "mean_3                   4.0084          3.4002          4.4620     1.0618\n",
      "median_3                 4.0363          3.4317          4.5468     1.1151\n",
      "std_3                    0.9463          0.7706          1.0989     0.3283\n",
      "1st_quart_3              4.2207          3.6321          4.6863     1.0542\n",
      "3rd_quart_3              4.1715          3.5385          4.6653     1.1268\n",
      "min_4                    0.0000          0.0000          0.0000     0.0000\n",
      "max_4                    2.1836          1.9790          2.3531     0.3741\n",
      "mean_4                   1.1663          1.0748          1.2254     0.1506\n",
      "median_4                 1.1454          1.0531          1.2011     0.1480\n",
      "std_4                    0.4579          0.4188          0.4850     0.0662\n",
      "1st_quart_4              0.8439          0.7751          0.8920     0.1169\n",
      "3rd_quart_4              1.5527          1.4299          1.6233     0.1934\n",
      "min_5                    6.1240          4.3976          7.5035     3.1059\n",
      "max_5                    5.7412          4.7623          6.5477     1.7854\n",
      "mean_5                   5.6753          4.4741          6.7804     2.3063\n",
      "median_5                 5.8138          4.5268          6.9182     2.3914\n",
      "std_5                    1.0246          0.8075          1.2177     0.4102\n",
      "1st_quart_5              6.0964          4.7900          7.2048     2.4148\n",
      "3rd_quart_5              5.5318          4.3424          6.5677     2.2253\n",
      "min_6                    0.0458          0.0000          0.0785     0.0785\n",
      "max_6                    2.5189          2.2601          2.7700     0.5099\n",
      "mean_6                   1.1545          1.0611          1.2164     0.1553\n",
      "median_6                 1.0859          0.9946          1.1487     0.1541\n",
      "std_6                    0.5177          0.4769          0.5451     0.0682\n",
      "1st_quart_6              0.7584          0.6938          0.8072     0.1134\n",
      "3rd_quart_6              1.5237          1.3940          1.6016     0.2076\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "std_estimates = total_features.std()\n",
    "confidence_intervals = {}\n",
    "\n",
    "for column in total_features.columns:\n",
    "    bootstrap_stds = []\n",
    "    for i in range(1000):\n",
    "        sample = total_features[column].sample(frac=1, replace=True)\n",
    "        bootstrap_stds.append(sample.std())\n",
    "\n",
    "    lower_percentile = (1 - 0.9) / 2\n",
    "    upper_percentile = 1 - lower_percentile\n",
    "    lower = np.percentile(bootstrap_stds, lower_percentile * 100)\n",
    "    upper = np.percentile(bootstrap_stds, upper_percentile * 100)\n",
    "\n",
    "    confidence_intervals[column] = (std_estimates[column], lower, upper)\n",
    "\n",
    "result_df = pd.DataFrame(confidence_intervals, \n",
    "                                      index=['Standard Deviation', \n",
    "                                             'CI Lower Bound', \n",
    "                                             'CI Upper Bound']).T.round(4)\n",
    "\n",
    "result_df['CI Length'] = result_df['CI Upper Bound'] - result_df['CI Lower Bound']\n",
    "\n",
    "print(\"The 90% bootstrap confidence interval for the standard deviation of each feature is:\")\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a6cc4",
   "metadata": {},
   "source": [
    "I would choose the following three most important time domain features:\n",
    "\n",
    "By side-by-side comparison:\n",
    "Among min, min_1 is the most important feature because its standard deviation is larger and the confidence interval is shorter, indicating significant changes in the data.\n",
    "Among max, max_5 is the most important feature because its standard deviation is high and the confidence interval is small, indicating that the data changes greatly and the estimate is stable.\n",
    "In mean, mean_1 is the most important feature because its standard deviation is large and the confidence interval is narrow, indicating that its estimation accuracy is high and there is large variation in the data.\n",
    "Among the median, median_1 is the most important feature because its standard deviation is high and the confidence interval is short, indicating better discrimination in the data.\n",
    "In Std, std_5 is the most important feature because its standard deviation is higher and the confidence interval is shorter, indicating greater data variation and better estimation accuracy.\n",
    "In Q1, 1st_quart_1 is the most important feature because it has a larger standard deviation and shorter confidence interval.\n",
    "In Q3, 3rd_quart_1 is the most important feature, exhibiting higher standard deviation and shorter confidence interval.\n",
    "\n",
    "From these seven statistics, I will further select the three most important time domain features. I will consider them based on the size of the standard deviation, the length of the confidence interval, and the representativeness of the statistics. Among them: mean_1, min_5 , and max_1 are the most representative\n",
    "\n",
    "The standard deviation of mean_1 is 5.3352 and the confidence interval is [4.7079, 5.8833]. The confidence interval is relatively narrow, indicating that the estimate is stable, and the mean is an important feature in describing the central trend of the data.\n",
    "\n",
    "min_5 (minimum value of the fifth time series): min_5 has a standard deviation of 6.1240 and a confidence interval of [4.3976, 7.5035]. Although the confidence interval is slightly wider, its standard deviation is larger, indicating that there is more variation in the data. The minimum value can reflect the lower bound of the data and is an important statistical feature.\n",
    "\n",
    "max_1 (the maximum value of the first time series): max_1 has a standard deviation of 4.3944 and a confidence interval of [3.3320, 5.2827]. The confidence intervals are narrow and the estimates are stable. The maximum value reflects the upper bound of the data, and the combination of the minimum value and the mean value can fully describe the range and distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4dec6d",
   "metadata": {},
   "source": [
    "ISLR 3.7.4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
