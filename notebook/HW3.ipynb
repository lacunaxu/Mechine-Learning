{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361be951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8155cd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data: 19 files\n",
      "Train Data: 69 files\n"
     ]
    }
   ],
   "source": [
    "#part (ab)\n",
    "folder_paths = [\"bending1\", \"bending2\", \"cycling\", \"lying\", \n",
    "                \"sitting\", \"standing\", \"walking\"]\n",
    "\n",
    "columns = ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', \n",
    "           'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "expected_columns = 7\n",
    "problematic_lines = []\n",
    "\n",
    "def detect_separator(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for _ in range(5):\n",
    "            next(file)\n",
    "        sample = file.read(200)  \n",
    "    \n",
    "    if ',' in sample:\n",
    "        return ','\n",
    "    else:\n",
    "        return '\\s+' \n",
    "\n",
    "def process_file(file_path):\n",
    "    sep = detect_separator(file_path)\n",
    "    df = pd.read_csv(file_path, comment='#', header=None, usecols=range(7), sep=sep)\n",
    "    df.columns = columns\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "for folder in folder_paths:\n",
    "    overall_path = os.getcwd()\n",
    "    new_path = os.path.dirname(overall_path)\n",
    "    path = os.path.join(new_path, 'data', 'AReM', folder)\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    for idx, file in enumerate(files):\n",
    "        \n",
    "        file_path = os.path.join(path, file)    \n",
    "        df = process_file(file_path)\n",
    "        if folder == \"bending1\" or folder == \"bending2\":\n",
    "            if idx < 2:\n",
    "                test_data.append(df)\n",
    "            else:\n",
    "                train_data.append(df)\n",
    "        else:\n",
    "            if idx < 3:\n",
    "                test_data.append(df)\n",
    "            else:\n",
    "                train_data.append(df)\n",
    "\n",
    "print(f\"Test Data: {len(test_data)} files\")\n",
    "print(f\"Train Data: {len(train_data)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e03a2e",
   "metadata": {},
   "source": [
    "c(i)\n",
    " Research what types of time-domain features are usually used in time series classification and list them (examples are minimum, maximum, mean, etc)\n",
    " \n",
    "Based on my research:\n",
    "\"In time series classification, time-domain features are extracted directly from the raw time series data. These features help capture the statistical properties and patterns of the data over time.\"\n",
    "\n",
    "Here are some time-domain features:\n",
    "Mean, Standard Deviation, Minimum and Maximum values, range, median, skewness, quartiles and IQR, Root Mean Square, Absolute Mean Difference, Entropy, trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb8390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_1  max_1  mean_1  median_1  std_1  1st_quart_1  3rd_quart_1  min_2  \\\n",
      "0  35.00  47.40   43.95     44.33   1.56        43.00        45.00    0.0   \n",
      "1  33.00  47.75   42.18     43.50   3.67        39.15        45.00    0.0   \n",
      "2  33.00  45.75   41.68     41.75   2.24        41.33        42.75    0.0   \n",
      "3  37.00  48.00   43.45     43.25   1.39        42.50        45.00    0.0   \n",
      "4  36.25  48.00   43.97     44.50   1.62        43.31        44.67    0.0   \n",
      "\n",
      "   max_2  mean_2  ...  std_5  1st_quart_5  3rd_quart_5  min_6  max_6  mean_6  \\\n",
      "0   1.70    0.43  ...   2.00        35.36        36.50    0.0   1.79    0.49   \n",
      "1   3.00    0.70  ...   3.85        30.46        36.33    0.0   2.18    0.61   \n",
      "2   2.83    0.54  ...   2.41        28.46        31.25    0.0   1.79    0.38   \n",
      "3   1.58    0.38  ...   2.49        22.25        24.00    0.0   5.26    0.68   \n",
      "4   1.50    0.41  ...   3.32        20.50        23.75    0.0   2.96    0.56   \n",
      "\n",
      "   median_6  std_6  1st_quart_6  3rd_quart_6  \n",
      "0      0.43   0.51         0.00         0.94  \n",
      "1      0.50   0.52         0.00         1.00  \n",
      "2      0.43   0.39         0.00         0.50  \n",
      "3      0.50   0.62         0.43         0.87  \n",
      "4      0.49   0.49         0.00         0.83  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "#2(ii)\n",
    "def extract_features(df):\n",
    "    features = {}\n",
    "    for i, col in enumerate(df.columns[1:]): \n",
    "        features[f'min_{i+1}'] = df[col].min()\n",
    "        features[f'max_{i+1}'] = df[col].max()\n",
    "        features[f'mean_{i+1}'] = df[col].mean()\n",
    "        features[f'median_{i+1}'] = df[col].median()\n",
    "        features[f'std_{i+1}'] = df[col].std()\n",
    "        features[f'1st_quart_{i+1}'] = df[col].quantile(0.25)\n",
    "        features[f'3rd_quart_{i+1}'] = df[col].quantile(0.75)\n",
    "    return features\n",
    "\n",
    "train_features = pd.DataFrame([extract_features(df) for df in train_data])\n",
    "test_features = pd.DataFrame([extract_features(df) for df in test_data])\n",
    "\n",
    "total_features = pd.concat([train_features, test_features], axis=0).round(2)\n",
    "print(total_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b1c88e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Normalized Features DataFrame:\n",
      "\n",
      "   min_1  max_1  mean_1  median_1  std_1  1st_quart_1  3rd_quart_1  min_2  \\\n",
      "0   0.73   0.66    0.83      0.85   0.20         0.80         0.67    0.0   \n",
      "1   0.69   0.68    0.75      0.81   0.48         0.64         0.67    0.0   \n",
      "2   0.69   0.60    0.73      0.74   0.29         0.73         0.59    0.0   \n",
      "3   0.77   0.69    0.81      0.80   0.18         0.78         0.67    0.0   \n",
      "4   0.76   0.69    0.83      0.85   0.21         0.81         0.66    0.0   \n",
      "\n",
      "   max_2  mean_2  ...  std_5  1st_quart_5  3rd_quart_5  min_6  max_6  mean_6  \\\n",
      "0   0.08    0.09  ...   0.04         1.00         1.00    0.0   0.00    0.03   \n",
      "1   0.15    0.15  ...   0.34         0.86         0.99    0.0   0.03    0.07   \n",
      "2   0.14    0.12  ...   0.11         0.80         0.83    0.0   0.00    0.00   \n",
      "3   0.07    0.08  ...   0.12         0.62         0.60    0.0   0.29    0.10   \n",
      "4   0.06    0.09  ...   0.26         0.57         0.59    0.0   0.10    0.06   \n",
      "\n",
      "   median_6  std_6  1st_quart_6  3rd_quart_6  \n",
      "0      0.00   0.08         0.00         0.11  \n",
      "1      0.02   0.08         0.00         0.12  \n",
      "2      0.00   0.00         0.00         0.00  \n",
      "3      0.02   0.15         0.19         0.09  \n",
      "4      0.02   0.06         0.00         0.08  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "Z-score Normalized Features DataFrame:\n",
      "   min_1  max_1  mean_1  median_1  std_1  1st_quart_1  3rd_quart_1  min_2  \\\n",
      "0   0.63   0.44    0.95      1.01  -0.81         0.98         0.74    0.0   \n",
      "1   0.42   0.52    0.62      0.86   0.39         0.35         0.74    0.0   \n",
      "2   0.42   0.06    0.52      0.53  -0.43         0.70         0.30    0.0   \n",
      "3   0.84   0.58    0.85      0.81  -0.91         0.90         0.74    0.0   \n",
      "4   0.76   0.58    0.95      1.04  -0.78         1.03         0.68    0.0   \n",
      "\n",
      "   max_2  mean_2  ...  std_5  1st_quart_5  3rd_quart_5  min_6  max_6  mean_6  \\\n",
      "0  -0.91   -0.66  ...  -1.51         3.55         3.27  -0.11  -1.87   -0.98   \n",
      "1  -0.65   -0.49  ...   0.31         2.74         3.24  -0.11  -1.71   -0.88   \n",
      "2  -0.69   -0.59  ...  -1.11         2.41         2.31  -0.11  -1.87   -1.08   \n",
      "3  -0.94   -0.69  ...  -1.03         1.39         0.99  -0.11  -0.48   -0.82   \n",
      "4  -0.95   -0.67  ...  -0.21         1.10         0.95  -0.11  -1.40   -0.92   \n",
      "\n",
      "   median_6  std_6  1st_quart_6  3rd_quart_6  \n",
      "0     -0.94  -1.05        -1.20        -0.79  \n",
      "1     -0.88  -1.03        -1.20        -0.75  \n",
      "2     -0.94  -1.28        -1.20        -1.08  \n",
      "3     -0.88  -0.83        -0.63        -0.84  \n",
      "4     -0.88  -1.08        -1.20        -0.87  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler_min_max = MinMaxScaler()\n",
    "normalized_min_max = scaler_min_max.fit_transform(total_features)\n",
    "normalized_data_min_max = pd.DataFrame(normalized_min_max, \n",
    "                                       columns=total_features.columns).round(2)\n",
    "\n",
    "scaler_z_score = StandardScaler()\n",
    "normalized_z_score = scaler_z_score.fit_transform(total_features)\n",
    "normalized_data_z_score = pd.DataFrame(normalized_z_score, \n",
    "                                       columns=total_features.columns).round(2)\n",
    "\n",
    "print(\"Min-Max Normalized Features DataFrame:\\n\")\n",
    "print(normalized_data_min_max.head())\n",
    "\n",
    "print(\"Z-score Normalized Features DataFrame:\")\n",
    "print(normalized_data_z_score.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd0439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 90% bootstrap confidence interval for the standard deviation of each feature is:\n",
      "             Standard Deviation  CI Lower Bound  CI Upper Bound  CI Length\n",
      "min_1                    9.5700          8.3447         10.8194     2.4747\n",
      "max_1                    4.3944          3.3122          5.3073     1.9951\n",
      "mean_1                   5.3352          4.6838          5.8540     1.1702\n",
      "median_1                 5.4402          4.7990          6.0415     1.2425\n",
      "std_1                    1.7721          1.5605          1.9458     0.3853\n",
      "1st_quart_1              6.1535          5.5601          6.6853     1.1252\n",
      "3rd_quart_1              5.1390          4.4030          5.8254     1.4224\n",
      "min_2                    0.0000          0.0000          0.0000     0.0000\n",
      "max_2                    5.0627          4.6144          5.3894     0.7750\n",
      "mean_2                   1.5743          1.3924          1.7046     0.3122\n",
      "median_2                 1.4121          1.2463          1.5315     0.2852\n",
      "std_2                    0.8843          0.8085          0.9466     0.1381\n",
      "1st_quart_2              0.9465          0.8399          1.0390     0.1991\n",
      "3rd_quart_2              2.1248          1.9003          2.2959     0.3956\n",
      "min_3                    2.9565          2.7452          3.1079     0.3627\n",
      "max_3                    4.8751          4.1493          5.5409     1.3916\n",
      "mean_3                   4.0084          3.4301          4.4825     1.0524\n",
      "median_3                 4.0363          3.4308          4.5547     1.1239\n",
      "std_3                    0.9463          0.7682          1.1139     0.3457\n",
      "1st_quart_3              4.2207          3.6441          4.6913     1.0472\n",
      "3rd_quart_3              4.1715          3.5111          4.6835     1.1724\n",
      "min_4                    0.0000          0.0000          0.0000     0.0000\n",
      "max_4                    2.1836          1.9660          2.3490     0.3830\n",
      "mean_4                   1.1663          1.0710          1.2233     0.1523\n",
      "median_4                 1.1454          1.0583          1.1985     0.1402\n",
      "std_4                    0.4579          0.4225          0.4867     0.0642\n",
      "1st_quart_4              0.8439          0.7796          0.8918     0.1122\n",
      "3rd_quart_4              1.5527          1.4282          1.6277     0.1995\n",
      "min_5                    6.1240          4.3748          7.5062     3.1314\n",
      "max_5                    5.7412          4.6975          6.5639     1.8664\n",
      "mean_5                   5.6753          4.3838          6.6962     2.3124\n",
      "median_5                 5.8138          4.6009          6.8533     2.2524\n",
      "std_5                    1.0246          0.8157          1.2033     0.3876\n",
      "1st_quart_5              6.0964          4.8063          7.2181     2.4118\n",
      "3rd_quart_5              5.5318          4.3399          6.6026     2.2627\n",
      "min_6                    0.0458          0.0000          0.0785     0.0785\n",
      "max_6                    2.5189          2.2433          2.7503     0.5070\n",
      "mean_6                   1.1545          1.0602          1.2179     0.1577\n",
      "median_6                 1.0859          0.9902          1.1457     0.1555\n",
      "std_6                    0.5177          0.4800          0.5436     0.0636\n",
      "1st_quart_6              0.7584          0.6924          0.8089     0.1165\n",
      "3rd_quart_6              1.5237          1.4084          1.6043     0.1959\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "std_estimates = total_features.std()\n",
    "confidence_intervals = {}\n",
    "\n",
    "for column in total_features.columns:\n",
    "    bootstrap_stds = []\n",
    "    for i in range(1000):\n",
    "        sample = total_features[column].sample(frac=1, replace=True)\n",
    "        bootstrap_stds.append(sample.std())\n",
    "\n",
    "    lower_percentile = (1 - 0.9) / 2\n",
    "    upper_percentile = 1 - lower_percentile\n",
    "    lower = np.percentile(bootstrap_stds, lower_percentile * 100)\n",
    "    upper = np.percentile(bootstrap_stds, upper_percentile * 100)\n",
    "\n",
    "    confidence_intervals[column] = (std_estimates[column], lower, upper)\n",
    "\n",
    "result_df = pd.DataFrame(confidence_intervals, \n",
    "                                      index=['Standard Deviation', \n",
    "                                             'CI Lower Bound', \n",
    "                                             'CI Upper Bound']).T.round(4)\n",
    "\n",
    "result_df['CI Length'] = result_df['CI Upper Bound'] - result_df['CI Lower Bound']\n",
    "\n",
    "print(\"The 90% bootstrap confidence interval for the standard deviation of each feature is:\")\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a6cc4",
   "metadata": {},
   "source": [
    "I would choose the following three most important time domain features:\n",
    "\n",
    "By side-by-side comparison:\n",
    "Among min, min_1 is the most important feature because its standard deviation is larger and the confidence interval is shorter, indicating significant changes in the data.\n",
    "Among max, max_5 is the most important feature because its standard deviation is high and the confidence interval is small, indicating that the data changes greatly and the estimate is stable.\n",
    "In mean, mean_1 is the most important feature because its standard deviation is large and the confidence interval is narrow, indicating that its estimation accuracy is high and there is large variation in the data.\n",
    "Among the median, median_1 is the most important feature because its standard deviation is high and the confidence interval is short, indicating better discrimination in the data.\n",
    "In Std, std_5 is the most important feature because its standard deviation is higher and the confidence interval is shorter, indicating greater data variation and better estimation accuracy.\n",
    "In Q1, 1st_quart_1 is the most important feature because it has a larger standard deviation and shorter confidence interval.\n",
    "In Q3, 3rd_quart_1 is the most important feature, exhibiting higher standard deviation and shorter confidence interval.\n",
    "\n",
    "From these seven statistics, I will further select the three most important time domain features. I will consider them based on the size of the standard deviation, the length of the confidence interval, and the representativeness of the statistics. Among them: mean_1, min_5 , and max_1 are the most representative\n",
    "\n",
    "The standard deviation of mean_1 is 5.3352 and the confidence interval is [4.7079, 5.8833]. The confidence interval is relatively narrow, indicating that the estimate is stable, and the mean is an important feature in describing the central trend of the data.\n",
    "\n",
    "min_5 (minimum value of the fifth time series): min_5 has a standard deviation of 6.1240 and a confidence interval of [4.3976, 7.5035]. Although the confidence interval is slightly wider, its standard deviation is larger, indicating that there is more variation in the data. The minimum value can reflect the lower bound of the data and is an important statistical feature.\n",
    "\n",
    "max_1 (the maximum value of the first time series): max_1 has a standard deviation of 4.3944 and a confidence interval of [3.3320, 5.2827]. The confidence intervals are narrow and the estimates are stable. The maximum value reflects the upper bound of the data, and the combination of the minimum value and the mean value can fully describe the range and distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4dec6d",
   "metadata": {},
   "source": [
    "ISLR 3.7.4\n",
    "(a) Since the cubic regression model is more complex and has more parameters than the linear regression model, the cubic regression model is able to fit the data more flexibly. So even if the true relationship is linear, the cubic model may still \"overfit\" the training data, resulting in a lower training RSS. So I would expect the polynomial regression have lower training RSS than the linear regression.\n",
    "(b) For the test data, when the true relationship is linear, the cubic model introduces unnecessary complexity because it overfits the training data, so for the test set, the cubic model will have a higher test RSS, while the test RSS of the linear regression is expected to be lower than the test RSS of the cubic regression because the linear regression model is simpler and better reflects the true linear relationship.\n",
    "(c) Polynomial regression has lower training RSS than the linear fit. Since the cubic regression model is more complex and has more parameters than the linear regression model, the cubic regression model is able to fit the data more flexibly. And when we assume that the true relationship between x and y is not linear, we need a more complex and flexible cubic regression model to give us a better fit for nonlinear data.\n",
    "(d) This depends on the degree of nonlinearity, so we do not have enough information to confirm which one is better. For more complex linear relationships between x and y, the cubic regression model may perform better than the linear regression model on the test set, so the test RSS of cubic regression may be lower than that of linear regression. However, if the degree of nonlinearity between x and y is very low, the cubic regression model will be similar to the answer in (b). They will overfit the training data, so in this case the test RSS of linear regression is expected to be lower than that of cubic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
